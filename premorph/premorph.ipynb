{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from collections import defaultdict\n",
    "from random import sample\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import display\n",
    "from estnltk.rewriting.premorph.morph_analyzed_token import MorphAnalyzedToken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pronouns\n",
    "Find all tokens that either `Vabamorf` or `MorphAnalyzedToken` thinks is pronoun. Save the results in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7418 pronoun-like tokens written to the file results/pronouns.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>vabamorf</th>\n",
       "      <th>is_pronoun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-END-</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-Esimene</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-Esimese</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-Esimeses</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-Esimest</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-Iga</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-Igal</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-Igalt</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-Ise</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-Keda</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token vabamorf is_pronoun\n",
       "0      -END-    False       True\n",
       "1   -Esimene     True       True\n",
       "2   -Esimese     True       True\n",
       "3  -Esimeses     True       True\n",
       "4   -Esimest     True       True\n",
       "5       -Iga     True       True\n",
       "6      -Igal     True       True\n",
       "7     -Igalt     True       True\n",
       "8       -Ise     True       True\n",
       "9      -Keda     True       True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronouns = []\n",
    "with open('../temp/wordlist') as in_f:\n",
    "    for i, line in enumerate(in_f):\n",
    "        token = MorphAnalyzedToken(line.strip())\n",
    "        if 'P' in token._part_of_speeches or token.normal.is_pronoun:\n",
    "            pronouns.append({'token': token, \n",
    "                             'vabamorf': 'P' in token._part_of_speeches,\n",
    "                             'is_pronoun': token.normal.is_pronoun})\n",
    "            if len(pronouns) % 500 == 1:\n",
    "                clear_output()\n",
    "                display('{} {}'.format(i, token))\n",
    "\n",
    "df = pandas.DataFrame.from_records(pronouns, columns=['token', 'vabamorf', 'is_pronoun'])\n",
    "out_file = 'results/pronouns.csv'\n",
    "df.to_csv(out_file, index=False)\n",
    "clear_output()\n",
    "print(len(pronouns), 'pronoun-like tokens written to the file', out_file)\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting tokens\n",
    "\n",
    "A token is interesting if\n",
    " - it contains a hyphen '`-`',\n",
    " - it is a word if hyphens are removed,\n",
    " - all hyphen-separated parts are words,\n",
    " - it contains a hyphen-separated part that consists of 2 or 3 letters and\n",
    " - it is not a pronoun.\n",
    "\n",
    "Being a word is defined by `Token.is_word` and being a pronoun is defined by `Token.is_pronoun`.\n",
    " \n",
    "Find all interesting tokens and save the results in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12204 interesting tokens written to the file results/interesting_tokens.txt\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "with open('../temp/wordlist', 'r', encoding='utf_8') as in_f:\n",
    "    for i, line in enumerate(in_f):\n",
    "        if '-' in line:\n",
    "            token = Token(line.strip())\n",
    "            parts = token.split('-')\n",
    "            if not any(1<len(part)<4 and part.isalpha() for part in parts):\n",
    "                continue\n",
    "            if not token.replace('-', '').is_word:\n",
    "                continue\n",
    "            if not all(part.is_word for part in parts):\n",
    "                continue\n",
    "            if token.is_pronoun:\n",
    "                continue\n",
    "            result.append(token)\n",
    "            if len(result) % 501 == 1:\n",
    "                clear_output()\n",
    "                display('{} {}'.format(i, token))\n",
    "clear_output()\n",
    "out_file = 'results/interesting_tokens.txt'\n",
    "with open('results/interesting_tokens.txt', 'w', encoding='utf_8') as out_f:\n",
    "    for token in result:\n",
    "        print(token, file=out_f)\n",
    "\n",
    "print(len(result), 'interesting tokens written to the file', out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate and count interesting tokens by short words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661 short words written to the file results/short_words.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_word</th>\n",
       "      <th>support</th>\n",
       "      <th>examples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>maa</td>\n",
       "      <td>580</td>\n",
       "      <td>[maa-algkoolid, maa-aluseid, maa-arstidele]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>töö</td>\n",
       "      <td>474</td>\n",
       "      <td>[töö-lubaduste, töö-otsijatele, võidu-töö]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>elu</td>\n",
       "      <td>408</td>\n",
       "      <td>[elu-asemepoliitikas, elu-stiili, elu-viisi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>üle</td>\n",
       "      <td>404</td>\n",
       "      <td>[üle-eestine, üle-kümne-tuhandeliste, üle-tree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>ühe</td>\n",
       "      <td>325</td>\n",
       "      <td>[ühe-kaheaastases, ühe-kahekorruselist, ühe-mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>tee</td>\n",
       "      <td>262</td>\n",
       "      <td>[tee-ehituslepingute, tee-ehitusprojektist, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>vee</td>\n",
       "      <td>262</td>\n",
       "      <td>[vee-eelnõu, vee-efektiga, vee-motoklubi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>aja</td>\n",
       "      <td>259</td>\n",
       "      <td>[aja-kirjanikule, aja-lool, ülemineku-aja]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>ja</td>\n",
       "      <td>243</td>\n",
       "      <td>[informaatika-ja, lae-ja-lase, silma-ja]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>pea</td>\n",
       "      <td>233</td>\n",
       "      <td>[pea-staabist, pea-toimetajakonkurss, pea-tree...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    short_word  support                                           examples\n",
       "239        maa      580        [maa-algkoolid, maa-aluseid, maa-arstidele]\n",
       "58         töö      474         [töö-lubaduste, töö-otsijatele, võidu-töö]\n",
       "336        elu      408       [elu-asemepoliitikas, elu-stiili, elu-viisi]\n",
       "65         üle      404  [üle-eestine, üle-kümne-tuhandeliste, üle-tree...\n",
       "607        ühe      325  [ühe-kaheaastases, ühe-kahekorruselist, ühe-mi...\n",
       "531        tee      262  [tee-ehituslepingute, tee-ehitusprojektist, te...\n",
       "622        vee      262          [vee-eelnõu, vee-efektiga, vee-motoklubi]\n",
       "360        aja      259         [aja-kirjanikule, aja-lool, ülemineku-aja]\n",
       "353         ja      243           [informaatika-ja, lae-ja-lase, silma-ja]\n",
       "308        pea      233  [pea-staabist, pea-toimetajakonkurss, pea-tree..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_words = defaultdict(list)\n",
    "\n",
    "with open('results/interesting_tokens.txt', 'r', encoding='utf_8') as in_f:\n",
    "    for line in in_f:\n",
    "        token = line.strip()\n",
    "        parts = token.split('-')\n",
    "        for part in parts:\n",
    "            if 1<len(part)<4 and part.isalpha():\n",
    "                short_words[part].append(token)\n",
    "table = []\n",
    "for word, examples in short_words.items():\n",
    "    table.append({'short_word': word,\n",
    "                  'support': len(examples),\n",
    "                  'examples': sorted(sample(examples,min(len(examples), 3)))})\n",
    "\n",
    "table = pandas.DataFrame.from_records(table, columns=['short_word', 'support', 'examples'])\n",
    "table = table.sort_values(\"support\", ascending=False)\n",
    "out_file = 'results/short_words.csv'\n",
    "table.to_csv(out_file, index=False)\n",
    "print(len(table), 'short words written to the file', out_file)\n",
    "table[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
