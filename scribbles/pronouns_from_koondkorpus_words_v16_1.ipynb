{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection `koondkorpus_words_v16_1`\n",
    "\n",
    "This collection is created with the script [create_koondkorpus_words_v16_1.py](create_koondkorpus_words_v16_1.py) (commit `ebc119761de38bd86028f6e9ee96df70e36d7a9d`).\n",
    "\n",
    "For every word in `v16_1_words` layer of `koondkorpus_base` collection there is an element in the `koondkorpus_words_v16_1` collection. This element is a sentence that contains the word with `target` layer that marks the position of the word in the sentence and has the `normalized_form` attribute. The word text string and the normalized form is also saved as the collection metadata in the `word` and `normalized_form` columns.\n",
    "\n",
    "There are 5350453 objects in the collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:db.py:1222: connecting to host: 'postgres.keeleressursid.ee', port: '5432', dbname: 'estonian-text-corpora', user: 'liisitor'\n",
      "INFO:db.py:1234: role: 'estonian_text_corpora_create'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liisi/anaconda3/envs/est1.6/lib/python3.5/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from estnltk.storage import PostgresStorage\n",
    "\n",
    "storage = PostgresStorage(pgpass_file='~/.pgpass',\n",
    "                          dbname='estonian-text-corpora',\n",
    "                          schema='estonian_text_corpora',\n",
    "                          role='estonian_text_corpora_create')\n",
    "\n",
    "collection = storage.get_collection('koondkorpus_words_v16_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a collection iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_collection = collection.select(layers=['target'], collection_meta=['word', 'normalized_form'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get pronouns from collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a dict of pronouns where key is normalized pronoun and values are analyses.\n",
    "\n",
    "`\"pronoun\": [{analysis}, {analysis}]`, where `analysis == {{collect_info}, {pronoun_analysis}}`\n",
    "\n",
    "`collect_info` - all information from collection\n",
    "\n",
    "`pronoun_analysis` - pronountagger output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from estnltk.taggers import VabamorfAnalyzer\n",
    "from estnltk.taggers import PostMorphAnalysisTagger\n",
    "from estnltk.taggers import PronounTypeTagger\n",
    "from collections import defaultdict\n",
    "from estnltk import Text\n",
    "\n",
    "pronoun_tagger = PronounTypeTagger()\n",
    "morph_analyzer = VabamorfAnalyzer()\n",
    "postanalysis_tagger = PostMorphAnalysisTagger()\n",
    "\n",
    "pronouns = defaultdict(list)\n",
    "\n",
    "for key, text, meta in iter_collection:\n",
    "    text.analyse('segmentation')\n",
    "    text.tag_layer(['tokens', 'compound_tokens'])\n",
    "    morph_analyzer.tag(text)\n",
    "    postanalysis_tagger.retag(text)\n",
    "    pronoun_tagger.tag(text)\n",
    "    for word in text.pronoun_type:\n",
    "        if word.start == text.target.start:  # kas lauses oleva s천na algus on sama, mis target s천nal\n",
    "            for i, pos in enumerate(word.partofspeech):\n",
    "                if pos == 'P':\n",
    "                    collect = {'key': key,\n",
    "                               'text': text.text,\n",
    "                               'meta_norm': meta['normalized_form'],\n",
    "                               'meta_word': meta['word'],\n",
    "                               'target': text.target.text,\n",
    "                               'target_start': text.target.start,\n",
    "                               'target_end': text.target.end}\n",
    "                    pron_analysis = {\n",
    "                        'pron_lemma': word.lemma[i],\n",
    "                        'pron_form': word.form[i],\n",
    "                        'pron_pos': pos,\n",
    "                        'pron_type': word.pronoun_type[i],\n",
    "                        'pron_root': word.root_tokens[i]}\n",
    "\n",
    "                    if pronouns[word.text] == []:\n",
    "                        pronouns[word.text].append({'collect_info': collect, 'pronoun_analysis': pron_analysis})\n",
    "                    else:\n",
    "                        analysis_exists = False\n",
    "                        for analysis in pronouns[word.text]:\n",
    "                            if pron_analysis == analysis['pronoun_analysis']:\n",
    "                                analysis_exists = True\n",
    "                                continue\n",
    "                            if not analysis_exists:\n",
    "                                pronouns[word.text].append({'collect_info': collect, 'pronoun_analysis': pron_analysis})\n",
    "                                analysis_exists = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('koond16pronouns.json', 'w') as f:\n",
    "    for chunk in json.JSONEncoder().iterencode(pronouns):\n",
    "        f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('koond16pronouns.json', 'r') as inf:\n",
    "    pronouns_from_json = json.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mingi\n",
      "analysis_1: {'collect_info': {'meta_norm': None, 'target': 'mingi', 'target_start': 46, 'target_end': 51, 'key': '931', 'text': 'See oli juba 2000. aasta algus , kohvikus oli mingi moe체ritus .', 'meta_word': 'mingi'}, 'pronoun_analysis': {'pron_pos': 'P', 'pron_lemma': 'mingi', 'pron_form': 'sg g', 'pron_root': ['mingi'], 'pron_type': ['indef']}}\n",
      "\n",
      "analysis_2: {'collect_info': {'meta_norm': None, 'target': 'mingi', 'target_start': 46, 'target_end': 51, 'key': '931', 'text': 'See oli juba 2000. aasta algus , kohvikus oli mingi moe체ritus .', 'meta_word': 'mingi'}, 'pronoun_analysis': {'pron_pos': 'P', 'pron_lemma': 'mingi', 'pron_form': 'sg n', 'pron_root': ['mingi'], 'pron_type': ['indef']}}\n",
      "\n",
      "analysis_3: {'collect_info': {'meta_norm': None, 'target': 'mingi-17', 'target_start': 12, 'target_end': 20, 'key': '5300295', 'text': 'bad_blondy: mingi-17', 'meta_word': 'mingi-17'}, 'pronoun_analysis': {'pron_pos': 'P', 'pron_lemma': 'mingi', 'pron_form': 'sg n', 'pron_root': ['mingi'], 'pron_type': ['indef']}}\n",
      "\n",
      "analysis_4: {'collect_info': {'meta_norm': None, 'target': 'mingi-tibi', 'target_start': 11, 'target_end': 21, 'key': '5319285', 'text': 'uudu: tere mingi-tibi', 'meta_word': 'mingi-tibi'}, 'pronoun_analysis': {'pron_pos': 'P', 'pron_lemma': 'mingi', 'pron_form': 'sg n', 'pron_root': ['mingi'], 'pron_type': ['indef']}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, values in sorted(pronouns_from_json.items()):\n",
    "    if key == 'mingi':\n",
    "        print(key)\n",
    "        for i, value in enumerate(values):\n",
    "            print(\"analysis_%s: %s\\n\" % (i+1, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pronoun_analysis_16_new.txt', 'a') as m:\n",
    "    for key, values in sorted(pronouns_from_json.items()):\n",
    "        m.write(\"%s %s\\n\" % (key, [[value['pronoun_analysis']['pron_lemma'], value['pronoun_analysis']['pron_pos'], value['pronoun_analysis']['pron_form'], value['pronoun_analysis']['pron_type']] for value in values]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
